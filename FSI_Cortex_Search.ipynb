{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18239d9e-e562-47a0-9b99-d83d089db6f2",
   "metadata": {
    "name": "md_steup",
    "collapsed": false
   },
   "source": "# **SNOWFLAKE CORTEX SEARCH FINANCIAL SERVICES DEMO**\n## Authors: John Heisler, Garrett Frere\nIn this demo, using Snowflake Cortex (https://www.snowflake.com/en/data-cloud/cortex/), we will build an RAG-powered chat interface using Cortex Search and Streamlit in Snowflake.\n\n### RAG\nWe'll learn how to extract raw text from a PDF, chunk the raw text, perform prompt engineering, pass custom prompts and data to a large language model, and use Cortex Search to both handle our embeddings and retreval all without leaving Snowflake.\n\nSpecifically, we will be taking on the role of an AI Engineer who is working closely with a portfolio team at an asset manager. The portfolio team would like to deepend their comprehension of Federal Open Market Committee (FOMC) statements and meeting minutes. The FOMC determines the direction of monetary policy by directing open market operations. Ultimately the portfolio team would like an interface to ask and answer specific questions of a document wihtout searching throughout the text.\n\nIn this section we'll: \n1. Create a new table function to extract and chunk the raw text from the statements and meeting minutes (which are in pdfs).\n2. Chunk and load raw text from the statements and minutes into our table.\n3. Create a Cortex Search Service to handle the vectorization and search functionlity.\n4. Create a Chat interface with Streamlit in Snowflake.\n\nThis is meant to be a companion to the FSI_Cortex_AI_Pipeline.ipynb also in this repository. \n\n# RAG\n![RAG](https://publish-p57963-e462109.adobeaemcloud.com/adobe/dynamicmedia/deliver/dm-aid--7e5d3595-a32c-44de-86ca-cfa2883d475e/rag1.png?quality=85&width=1920&preferwebp=true 'CORTEX SEARCH')\n\n\n# RAG with Cortex Search\n![Cortex Search](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/img/1d96fe59a89a3ac5.png)\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "ed6def1c-7dbb-4b60-baf9-a2562eab5496",
   "metadata": {
    "collapsed": false,
    "name": "md_manual_data_loads"
   },
   "source": "# üõë BEFORE YOU START üõë\n\n**Run the 1_SQL_SETUP_FOMC.sql script**\n"
  },
  {
   "cell_type": "markdown",
   "id": "a8b3df6a-2005-444f-9f36-edf5cc0d808a",
   "metadata": {
    "name": "md_chunking_function",
    "collapsed": false
   },
   "source": "# RAG - Step 1 \n## Chunking Function\nEarlier, we created a function that pulled all of the text out of a PDF. Now, we'll do somethign very similar but we're going to break the text up into chunks for fuel our RAG interface. "
  },
  {
   "cell_type": "code",
   "id": "8b10280d-bdaf-4b11-b194-932f0fc0015f",
   "metadata": {
    "language": "sql",
    "name": "sql_chunking_function"
   },
   "outputs": [],
   "source": "--create a function to chunk the pdfs\nCREATE OR REPLACE FUNCTION PDF_TEXT_CHUNKER(file_url string)\nRETURNS TABLE (chunk varchar)\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.9'\nHANDLER = 'pdf_text_chunker'\nPACKAGES = ('snowflake-snowpark-python','PyPDF2', 'langchain')\nAS\n$$\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom snowflake.snowpark.files import SnowflakeFile\nimport PyPDF2, io\nimport logging\nimport pandas as pd\n\nclass pdf_text_chunker:\n\n    def read_pdf(self, file_url: str) -> str:\n    \n        logger = logging.getLogger(\"udf_logger\")\n        logger.info(f\"Opening file {file_url}\")\n    \n        with SnowflakeFile.open(file_url, 'rb') as f:\n            buffer = io.BytesIO(f.readall())\n            \n        reader = PyPDF2.PdfReader(buffer)   \n        text = \"\"\n        for page in reader.pages:\n            try:\n                text += page.extract_text().replace('\\n', ' ').replace('\\0', ' ')\n            except:\n                text = \"Unable to Extract\"\n                logger.warn(f\"Unable to extract from file {file_url}, page {page}\")\n        \n        return text\n\n    def process(self,file_url: str):\n\n        text = self.read_pdf(file_url)\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 500, #Adjust this as you see fit\n            chunk_overlap  = 50, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n        )\n    \n        chunks = text_splitter.split_text(text)\n        df = pd.DataFrame(chunks, columns=['chunks'])\n        \n        yield from df.itertuples(index=False, name=None)\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3702f0db-5abc-491a-aa57-891896cfa10b",
   "metadata": {
    "name": "md_chunking",
    "collapsed": false
   },
   "source": "# RAG - STEP 2 \n## Build the Chunk Table\nUsing our newly create chunking table function, being in and chunk all of the documents."
  },
  {
   "cell_type": "code",
   "id": "026b24dc-1849-4c64-b0b3-337bc85781be",
   "metadata": {
    "language": "sql",
    "name": "sql_chunk_pdfs",
    "collapsed": false
   },
   "outputs": [],
   "source": "TRUNCATE TABLE PDF_CHUNKS;\nINSERT INTO PDF_CHUNKS (ID, FULL_TEXT_FK, RELATIVE_PATH, FILE_DATE, CHUNK)\nWITH chunk_cte AS (\nSELECT \n    FED_PDF_CHUNK_SEQUENCE.NEXTVAL as ID,\n    relative_path,\n    REPLACE(func.chunk,'''' ,'') as chunk,\nFROM \n    directory(@FED_PDF),\n    TABLE(pdf_text_chunker(build_scoped_file_url(@FED_PDF, relative_path))) as func\n)\n\nSELECT\n    cte.ID,\n    pft.ID,\n    cte.relative_path, \n    pft.file_date,\n    cte.chunk\nFROM\n    chunk_cte cte\nLEFT JOIN \n    PDF_FULL_TEXT pft\n    on  cte.relative_path = pft.RELATIVE_PATH;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b95175d3-e4c5-47aa-b21a-e504b6ac6e7f",
   "metadata": {
    "collapsed": false,
    "name": "md_search_service"
   },
   "source": "# RAG - Step 3\n## Create a Cortex Search Service\n\nCortex Search enables low-latency, high-quality ‚Äúfuzzy‚Äù search over your Snowflake data. Cortex Search powers a broad array of search experiences for Snowflake users including Retrieval Augmented Generation (RAG) applications leveraging Large Language Models (LLMs).\n\nWe'll use this service later to power our RAG application. "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576cec9-f819-4598-951a-b660a2a977e9",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql_search_service"
   },
   "outputs": [],
   "source": "--create a cortex Search Service \nCREATE OR REPLACE CORTEX SEARCH SERVICE SRCH_FED\nON CHUNK\nATTRIBUTES ID, FILE_DATE\nWAREHOUSE = GEN_AI_FSI_WH\nTARGET_LAG = '1 day'\nAS (\n    SELECT \n        ID,\n        FILE_DATE::string as FILE_DATE,\n        CHUNK AS CHUNK  \nFROM PDF_CHUNKS);"
  },
  {
   "cell_type": "markdown",
   "id": "37b0e39c-62d3-494f-95ba-8279a88eedae",
   "metadata": {
    "collapsed": false,
    "name": "md_search"
   },
   "source": "# RAG - STEP 4\n## FOMC Chat Interface\n* We're leveraging our Cortex Search service enabling users to ask targeted questions of the documents in their stage.\n* a robust chat interface could be built to handle this, for the demo, we have a bare bones interaction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395e7b9-fb06-4cb0-ac10-ff007345e0fa",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cs_py_service"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.core import Root\nimport streamlit as st\nimport json5 as json\nimport pandas as pd\n#import snowflake.snowpark.modin.plugin\n\n#get our session\nsession = get_active_session()\n\n# access search service through Python API\nroot = Root(session)\nsearch_service = (root\n                  .databases[\"GEN_AI_FSI\"]\n                  .schemas[\"FOMC\"]\n                  .cortex_search_services[\"SRCH_FED\"]    \n)\n\n#create a function to generate response\ndef complete_cs(model_name, prompt):\n    cmd = f\"\"\"SELECT SNOWFLAKE.CORTEX.TRY_COMPLETE('{model_name}','{prompt}') as response\"\"\"\n    df_response = session.sql(cmd).collect()\n    response = df_response[0].RESPONSE\n    return response\n\n\n#get FOMC files\ndatabase = 'GEN_AI_FSI'\nschema = 'FOMC'\n\n#USER INPUT: select time frame\nquery_document_dates = f\"\"\"SELECT DISTINCT FILE_DATE FROM {database}.{schema}.PDF_CHUNKS order by file_date desc;\"\"\"\ndf_document_dates = session.sql(query_document_dates).to_pandas()\n\n#USER INPUT: select model\nquery_models = f\"\"\"SELECT MODEL FROM {database}.{schema}.MODELS\"\"\"\ndf_models = session.sql(query_models).to_pandas()\n\n#USER INPUT: display\ncol1, col2 = st.columns(2)\nwith col1:\n    user_input_date = st.selectbox(\"Select Document Date\", df_document_dates, key=\"CS_date_select_box\")\nwith col2: \n    user_input_model = st.selectbox(\"Select Model\", df_models, key=\"CS_model_select_box\")\n\n#Generate a response\nuser_input_question = st.text_input(\"Ask me a question\")\n\nask= st.button(\"Ask\", key = \"button_ask\")\nif ask: \n    #get the cunks that are relevant to the question\n    response = search_service.search(\n        user_input_question,\n        columns=[\"ID\", \"FILE_DATE\", \"CHUNK\"],\n        filter={\"@eq\": {\"FILE_DATE\": f\"\"\"{user_input_date}\"\"\"} },\n        limit=5,\n    ).to_json()\n\n    #st.json(response)\n    # Parse the JSON5 string\n    context_chunks = json.loads(response)\n    \n    #transform the data into a single string\n    context_full = \"\"\n    for chunk in context_chunks['results']:\n        context_full += chunk['CHUNK'] + \" \"\n\n    #build our prompt\n    cs_prompt = f'''\n            Role: You are an expert Senior Economist deeply knowledgeable on Federal Reserve documents and guidance including FOMC or Federal Open Market Committee \n            meeting minutes and communications. You are an expert in interpreting and answering investment-related questions based on meeting minutes and communications \n            which you are provided as context with each question.\n            \n            Data: You are provided with relevant text of a Federal Reserve Guidance or FOMC meeting notes relenavt to the question asked. \n            These meeting notes are generally released before the Federal Reserve takes action on economic policy.\n            \n            Task: Follow these instructions,\n            1) Answer the question based on the context. \n            2) Be terse and do not consider information outside what you have been provided in the question and context.\n            \n            Output: produce thourough, valid, gramtically correct, and concise response in a professional tone. Please do not preface your response. also provide the document and location you used to answer the question.\n            Context: {context_full}\n            Question: Based on documents released on this date {user_input_date}, {user_input_question} \n            '''\n\n    data = complete_cs(user_input_model, cs_prompt)\n    \n    with st.chat_message(\"model\", avatar =\"assistant\"):\n        st.write(data)"
  }
 ]
}
